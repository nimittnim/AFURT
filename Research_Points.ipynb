{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting patent Info \n",
    "\n",
    "def get_patent_info_google_patents(patent_number):\n",
    "\n",
    "    google_patents_url = f\"https://patents.google.com/patent/{patent_number}\"\n",
    "    response = requests.get(google_patents_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    patent_info = {}\n",
    "\n",
    "    patent_info[\"Title\"] = soup.find(\"meta\", attrs={\"name\": \"DC.title\"})[\"content\"]\n",
    "    patent_info[\"Publication Date\"] = soup.find(\"meta\", attrs={\"name\": \"DC.date\", \"scheme\": \"issue\"})[\"content\"]\n",
    "    inventors = soup.find_all(\"meta\", attrs={\"name\": \"DC.contributor\", \"scheme\": \"inventor\"})\n",
    "    patent_info[\"Inventors\"] = [inventor[\"content\"] for inventor in inventors]\n",
    "    patent_info[\"Application Number\"] = soup.find(\"meta\", attrs={\"name\": \"citation_patent_application_number\"})[\"content\"]\n",
    "    patent_info[\"PDF URL\"] = soup.find(\"meta\", attrs={\"name\": \"citation_pdf_url\"})[\"content\"]\n",
    "\n",
    "    return patent_info\n",
    "\n",
    "# Processing to get country code\n",
    "\n",
    "def get_country_code_from_patent_number(patent_number, country_codes):\n",
    "    code = patent_number[:2]\n",
    "    country_name = country_codes.get(code, \"Country not found\")\n",
    "\n",
    "    return code, country_name\n",
    "\n",
    "# Preprocessing Patent Number\n",
    "\n",
    "def process_patent_number(patent_number):\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', patent_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country Codes for Patents\n",
    "\n",
    "country_codes = {\n",
    "    \"AM\": \"Armenia\",\n",
    "    \"AP\": \"African Regional Industrial Property Organization\",\n",
    "    \"AR\": \"Argentina\",\n",
    "    \"AT\": \"Austria\",\n",
    "    \"AU\": \"Australia\",\n",
    "    \"BA\": \"Bosnia and Herzegovina\",\n",
    "    \"BE\": \"Belgium\",\n",
    "    \"BG\": \"Bulgaria\",\n",
    "    \"BR\": \"Brazil\",\n",
    "    \"BY\": \"Belarus\",\n",
    "    \"CA\": \"Canada\",\n",
    "    \"CH\": \"Switzerland\",\n",
    "    \"CL\": \"Chile\",\n",
    "    \"CN\": \"China\",\n",
    "    \"CO\": \"Colombia\",\n",
    "    \"CR\": \"Costa Rica\",\n",
    "    \"CS\": \"Czechoslovakia\",\n",
    "    \"CU\": \"Cuba\",\n",
    "    \"CY\": \"Cyprus\",\n",
    "    \"CZ\": \"Czech Republic\",\n",
    "    \"DD\": \"German Democratic Republic\",\n",
    "    \"DE\": \"Germany\",\n",
    "    \"DK\": \"Denmark\",\n",
    "    \"DO\": \"Dominican Republic\",\n",
    "    \"DZ\": \"Algeria\",\n",
    "    \"EA\": \"Eurasian Patent Organization\",\n",
    "    \"EC\": \"Ecuador\",\n",
    "    \"EE\": \"Estonia\",\n",
    "    \"EG\": \"Egypt\",\n",
    "    \"EM\": \"European Union Intellectual Property Office\",\n",
    "    \"EP\": \"European Patent Office\",\n",
    "    \"ES\": \"Spain\",\n",
    "    \"FI\": \"Finland\",\n",
    "    \"FR\": \"France\",\n",
    "    \"GB\": \"United Kingdom\",\n",
    "    \"GC\": \"Gulf Cooperation Council\",\n",
    "    \"GE\": \"Georgia\",\n",
    "    \"GR\": \"Greece\",\n",
    "    \"GT\": \"Guatemala\",\n",
    "    \"HK\": \"The Hong Kong Special Administrative Region of the Peopleâ€™s Republic of China\",\n",
    "    \"HN\": \"Honduras\",\n",
    "    \"HR\": \"Croatia\",\n",
    "    \"HU\": \"Hungary\",\n",
    "    \"ID\": \"Indonesia\",\n",
    "    \"IE\": \"Ireland\",\n",
    "    \"IL\": \"Israel\",\n",
    "    \"IN\": \"India\",\n",
    "    \"IS\": \"Iceland\",\n",
    "    \"IT\": \"Italy\",\n",
    "    \"JO\": \"Jordan\",\n",
    "    \"JP\": \"Japan\",\n",
    "    \"KE\": \"Kenya\",\n",
    "    \"KG\": \"Kyrgyzstan\",\n",
    "    \"KR\": \"Korea (South)\",\n",
    "    \"KZ\": \"Kazakhstan\",\n",
    "    \"LT\": \"Lithuania\",\n",
    "    \"LU\": \"Luxembourg\",\n",
    "    \"LV\": \"Latvia\",\n",
    "    \"MA\": \"Morocco\",\n",
    "    \"MC\": \"Monaco\",\n",
    "    \"MD\": \"Republic of Moldova\",\n",
    "    \"ME\": \"Montenegro\",\n",
    "    \"MN\": \"Mongolia\",\n",
    "    \"MO\": \"Macao\",\n",
    "    \"MT\": \"Malta\",\n",
    "    \"MW\": \"Malawi\",\n",
    "    \"MX\": \"Mexico\",\n",
    "    \"MY\": \"Malaysia\",\n",
    "    \"NI\": \"Nicaragua\",\n",
    "    \"NL\": \"Netherlands\",\n",
    "    \"NO\": \"Norway\",\n",
    "    \"NZ\": \"New Zealand\",\n",
    "    \"OA\": \"African Intellectual Property Organization\",\n",
    "    \"PA\": \"Panama\",\n",
    "    \"PE\": \"Peru\",\n",
    "    \"PH\": \"Philippines\",\n",
    "    \"PL\": \"Poland\",\n",
    "    \"PT\": \"Portugal\",\n",
    "    \"RO\": \"Romania\",\n",
    "    \"RS\": \"Serbia\",\n",
    "    \"RU\": \"Russian Federation\",\n",
    "    \"SA\": \"Saudi Arabia\",\n",
    "    \"SE\": \"Sweden\",\n",
    "    \"SG\": \"Singapore\",\n",
    "    \"SI\": \"Slovenia\",\n",
    "    \"SK\": \"Slovakia\",\n",
    "    \"SM\": \"San Marino\",\n",
    "    \"SU\": \"Soviet Union (USSR)\",\n",
    "    \"SV\": \"El Salvador\",\n",
    "    \"TH\": \"Thailand\",\n",
    "    \"TJ\": \"Tajikistan\",\n",
    "    \"TN\": \"Tunisia\",\n",
    "    \"TR\": \"Turkey\",\n",
    "    \"TT\": \"Trinidad and Tobago\",\n",
    "    \"TW\": \"Chinese Taipei\",\n",
    "    \"UA\": \"Ukraine\",\n",
    "    \"US\": \"United States of America\",\n",
    "    \"UY\": \"Uruguay\",\n",
    "    \"UZ\": \"Uzbekistan\",\n",
    "    \"VN\": \"Viet Nam\",\n",
    "    \"WO\": \"World Intellectual Property Organization (WIPO)\",\n",
    "    \"YU\": \"Yugoslavia / Serbia and Montenegro\",\n",
    "    \"ZA\": \"South Africa\",\n",
    "    \"ZM\": \"Zambia\",\n",
    "    \"ZW\": \"Zimbabwe\"\n",
    "}\n",
    "\n",
    "good_country_codes = [\"US\",\"EM\",\"RU\",\"KR\",\"AU\",\"JP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Journal Name and Authors From DOI number\n",
    "\n",
    "\n",
    "def get_journal_and_authors_from_doi(doi):\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            journal_name = data['message']['container-title'][0]  \n",
    "            authors = data['message']['author']\n",
    "            author_names = [f\"{author['given']} {author['family']}\" for author in authors]\n",
    "            return journal_name, author_names\n",
    "        else:\n",
    "            print(f\"Error 2. Status code: {response.status_code}\")\n",
    "            return None, []\n",
    "    except:\n",
    "        print(\"Error 1.\")\n",
    "        return None, []\n",
    "    \n",
    "# Getting the Link of Journal Data page from main ScimagoJR page\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "def search_page(journal_name):\n",
    "  if journal_name:\n",
    "    search_name=quote_plus(journal_name.strip())\n",
    "    search_url=f\"https://www.scimagojr.com/journalsearch.php?q={search_name}\"\n",
    "    r = requests.get(search_url)\n",
    "  else : \n",
    "    return None\n",
    "  if r.status_code==200:\n",
    "    soup = BeautifulSoup(r.content, 'html5lib')\n",
    "    try:\n",
    "      search_result=soup.find('div',class_=\"search_results\").find(\"a\")['href']\n",
    "    except TypeError:\n",
    "      search_result=None\n",
    "      print(journal_name,\"No results were found\")\n",
    "  else:\n",
    "    search_result=None\n",
    "    print(journal_name,'Page not found 0')\n",
    "  return search_result\n",
    "\n",
    "#Getting Quartile from Journal Data\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def quartile_list(search_result):\n",
    "  if search_result!=None:\n",
    "    result_url=f'https://www.scimagojr.com/{search_result}'\n",
    "    try:\n",
    "      response = requests.get(result_url)\n",
    "      soup = BeautifulSoup(response.text, 'html.parser')\n",
    "      quartile_th = soup.find('th', string='Quartile')\n",
    "      if quartile_th:\n",
    "        quartile_table = quartile_th.find_parent('table')\n",
    "        if quartile_table:\n",
    "            df = pd.read_html(result_url,match='Quartile')[0]\n",
    "            latest_rating_indices = df.groupby('Category').Year.agg('idxmax')\n",
    "            latest_ratings = df.loc[latest_rating_indices]\n",
    "            unique_ratings = latest_ratings['Quartile'].unique()\n",
    "        else:\n",
    "            unique_ratings=None\n",
    "            print(result_url, \"No quartile category found\")\n",
    "      else:\n",
    "            unique_ratings = None\n",
    "            print(result_url, \"No quartile found on page\")\n",
    "    except requests.HTTPError:\n",
    "      unique_ratings=None\n",
    "      print(result_url,\"Page not found\")\n",
    "  else:\n",
    "      unique_ratings=None\n",
    "  return unique_ratings\n",
    "\n",
    "# Getting DOI data from Google Spreadsheet\n",
    "\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import numpy as np\n",
    "def get_doi_list_from_spreadsheet(sheetName):\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\"facultyrecruitmentupgradation-5e06eb0edef3.json\", scope)\n",
    "\n",
    "    gc = gspread.authorize(credentials)\n",
    "    worksheet = gc.open(sheetName).sheet1\n",
    "\n",
    "    data = worksheet.get_all_records()\n",
    "    doi_dataframe = pd.DataFrame(data)\n",
    "    doi_check = np.array(doi_dataframe['CATEGORY']).tolist()\n",
    "    doi_done = 0\n",
    "    for i in range(len(doi_check)):\n",
    "        if doi_check[i] != '':\n",
    "            doi_done+=1\n",
    "    doi_data = doi_dataframe['DOI NUMBER']\n",
    "    doi_list = np.array(doi_data).tolist()\n",
    "    doi_list = doi_list[doi_done:]\n",
    "    for i in range(len(doi_list)):\n",
    "        if str(doi_list[i][0:15]) == \"https://doi.org\":\n",
    "            doi_list[i] = doi_list[i][16:]\n",
    "    return doi_list,doi_done\n",
    "sheetName = \"ApplicationsData\"\n",
    "\n",
    "\n",
    "# Post Processing Incorrect Entries\n",
    "\n",
    "def post_process_invalid_inputs(Journal_name,doi):\n",
    "    if Journal_name:\n",
    "        Journal_name = Journal_name.split(':')[0]\n",
    "        return Journal_name,doi\n",
    "    else :\n",
    "        return None,None\n",
    "    \n",
    "\n",
    "# Writing into Google Spreadsheet \n",
    "  \n",
    "def enter_into_sheet(df,sheetName,start):\n",
    "        scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "        credentials = ServiceAccountCredentials.from_json_keyfile_name('facultyrecruitmentupgradation-5e06eb0edef3.json', scope)\n",
    "        gc = gspread.authorize(credentials)\n",
    "        spreadsheet = gc.open(sheetName)\n",
    "\n",
    "        worksheet = spreadsheet.worksheet('Sheet1')  \n",
    "        worksheet.delete_rows(start, start + len(df) -1)\n",
    "        data_sheet = []\n",
    "        n = len(df)\n",
    "        for i in range(n):\n",
    "                data_sheet.append(list(map(str,np.array(df.loc[i]).tolist())))\n",
    "        worksheet.insert_rows(data_sheet, start)\n",
    "\n",
    "\n",
    "# Converting Quartile Cateqory to score\n",
    "\n",
    "def quartile_to_score(quartile):\n",
    "    if (quartile):\n",
    "        if ('Q1' in quartile):\n",
    "            return 2\n",
    "        elif ('Q2' in quartile or ('Q2' in quartile and 'Q1' in quartile)):\n",
    "            return 1\n",
    "        elif (quartile == 'Q3'):\n",
    "            return 0.25\n",
    "        else:\n",
    "            return 0\n",
    "    else :\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting book points \n",
    "\n",
    "def get_books_points(author_name, api_key):\n",
    "    base_url = \"https://www.googleapis.com/books/v1/volumes\"\n",
    "    params = {\n",
    "        \"q\": f\"inauthor:{author_name}\",\n",
    "        \"key\": api_key\n",
    "    }\n",
    "    book_sum=0\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "        if 'items' in data:\n",
    "            books = data['items']\n",
    "            no_of_books=len(books)\n",
    "            for book in books:\n",
    "                authors = book['volumeInfo'].get('authors', [])\n",
    "                no_of_authors=len(authors)\n",
    "                identifiers = book['volumeInfo'].get('industryIdentifiers', [])\n",
    "                no_of_chapters=len(identifiers)\n",
    "                if no_of_chapters<=0:\n",
    "                  continue\n",
    "                if no_of_authors<=0:\n",
    "                  continue\n",
    "                if no_of_chapters==1:\n",
    "                  book_factor= 1\n",
    "                else:\n",
    "                  book_factor=5\n",
    "                contribution=1/no_of_authors\n",
    "                book_sum+=(contribution*book_factor)\n",
    "            return book_sum\n",
    "        else:\n",
    "            print(f\"No books found by {author_name}\")\n",
    "            return\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transportation Research Record: Journal of the Transportation Research Board Page not found 0\n",
      "Paper Points :  0.25 \n",
      "Book Points :  1.0 \n",
      "Patent Points :  1.25\n",
      "2.5\n"
     ]
    }
   ],
   "source": [
    "# Research points\n",
    "\n",
    "def find_research_points(patent_list, doi_list, author_name ):\n",
    "\n",
    "    # Paper Points\n",
    "    # paper_contribution = 1/number of authors\n",
    "    paper_points =0\n",
    "    data=[]\n",
    "    for doi in doi_list:\n",
    "        journal_name, author_names = get_journal_and_authors_from_doi(doi)\n",
    "        search_result=search_page(journal_name)\n",
    "        quartile=quartile_list(search_result)\n",
    "\n",
    "        if quartile == None:\n",
    "            journal_name = post_process_invalid_inputs(journal_name,doi)[0]\n",
    "        search_result=search_page(journal_name)\n",
    "        quartile=quartile_list(search_result)\n",
    "        data.append([doi,journal_name,quartile,len(author_names)])\n",
    "\n",
    "    df=pd.DataFrame(data,columns=['DOI','Journal name','Category','No of authors'])\n",
    "    for i in range(len(df)):\n",
    "        quartile_score = quartile_to_score(df['Category'][i])\n",
    "        number_of_auth = df['No of authors'][i]\n",
    "        paper_contribution = 1/number_of_auth\n",
    "        paper_points += quartile_score*paper_contribution\n",
    "\n",
    "        \n",
    "\n",
    "    # Patent Points\n",
    "    \n",
    "    patent_points = 0\n",
    "    for patent_number in patent_list:\n",
    "        patent_number = process_patent_number(patent_number)\n",
    "        patent_info = get_patent_info_google_patents(patent_number)\n",
    "        if (patent_info):\n",
    "            number_of_inventors = len(patent_info['Inventors'])\n",
    "            good_patent_impact = 5\n",
    "            bad_patent_impact = 3\n",
    "            inventor_contribution = 1/number_of_inventors\n",
    "            country_code = get_country_code_from_patent_number(patent_number,country_codes)[0]\n",
    "            if (country_code in good_country_codes):\n",
    "                patent_points += good_patent_impact*(inventor_contribution)\n",
    "            else :\n",
    "                patent_points += bad_patent_impact*(inventor_contribution)\n",
    "        else :\n",
    "            patent_points += 0\n",
    "    \n",
    "    # Book Points\n",
    "    api_key = \"AIzaSyCEwBEmxq5Cc3HEZNmmy90eMLBfiiRCybE\"\n",
    "    book_points = get_books_points(author_name, api_key)\n",
    "    print('Paper Points : ',paper_points,\"\\nBook Points : \",book_points,\"\\nPatent Points : \",patent_points)\n",
    "    research_points = paper_points + patent_points + book_points\n",
    "    return research_points\n",
    "    \n",
    "patent_list = [\"US10666504\"]\n",
    "doi_list = [\"10.1177/03611981211064994\"]\n",
    "auth_name = \"Arup Lal Chakraborty\"\n",
    "print(find_research_points(patent_list, doi_list, auth_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
